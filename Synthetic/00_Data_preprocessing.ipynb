{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40917038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import random \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741a99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc (filename):\n",
    "    # Transforms images to a PyTorch Tensor\n",
    "    tensor_transform = transforms.ToTensor()\n",
    "  \n",
    "    # Download the MNIST Dataset\n",
    "    data_zip = np.load(filename)\n",
    "    dataset = torch.from_numpy(np.concatenate((data_zip['train_images'],data_zip['val_images'],\n",
    "                          data_zip['test_images']),0).astype(float)) \n",
    "\n",
    "    means = dataset.mean([1,2])\n",
    "    stds  = dataset.std([1,2])\n",
    "    mask = (stds!=0)\n",
    "\n",
    "    stds = stds[mask]\n",
    "    means = means[mask]\n",
    "    dataset = dataset[mask]\n",
    "\n",
    "    norm_tr = transforms.Compose([ transforms.Normalize(mean=means, std=stds, inplace=False) ])\n",
    "    dataset = norm_tr(dataset)\n",
    "    dataset = dataset.unsqueeze(1)\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e406b",
   "metadata": {},
   "source": [
    "### M1 and M2"
   ]
  },
   "source": [
    "# Download the MNIST Dataset\n",
    "dataset1 = pre_proc('organamnist.npz')\n",
    "dataset2 = pre_proc('organcmnist.npz')\n",
    "\n",
    "# Same images from each dataset \n",
    "ld2 = len(dataset2)\n",
    "dataset1 = dataset1[0:ld2,:,:,:]\n",
    "dataset2 = dataset2[:,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b67474",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afba2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = torch.utils.data.DataLoader(dataset = dataset1,\n",
    "                                     batch_size = 32,\n",
    "                                     shuffle = True)\n",
    "\n",
    "loader2 = torch.utils.data.DataLoader(dataset = dataset2,\n",
    "                                     batch_size = 32,\n",
    "                                     shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ab1806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, enc_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, 16, 3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(64* 5* 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, enc_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(enc_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64 * 5 * 5)\n",
    "            )\n",
    "        \n",
    "        self.unflatten = nn.Unflatten(1, (64, 5, 5))\n",
    "        \n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 16, 9),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 1, 12),  \n",
    "            nn.BatchNorm2d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_cnn(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219beaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_lin): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1600, bias=True)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(64, 5, 5))\n",
       "  (decoder_cnn): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(64, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(16, 16, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(16, 1, kernel_size=(12, 12), stride=(1, 1))\n",
       "    (10): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr= 0.001\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# Model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
    "encoder = Encoder(10)\n",
    "decoder = Decoder(10)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a330d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train().double()\n",
    "    decoder.train().double()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Move tensor to the proper device\n",
    "        image_batch = image_batch.to(device)\n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_batch)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "#         print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303cada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f93c81",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29145a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.save(encoder, './encoder.pt')\n",
    "# torch.save(decoder, './decoder.pt')\n",
    "encoder = torch.load('./encoder.pt')\n",
    "decoder = torch.load('./decoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d032f2c",
   "metadata": {},
   "source": [
    "## Image feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e832011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deep features using trained encoder\n",
    "d = 10\n",
    "img_feat1 = np.zeros((ld2,d))\n",
    "img_feat2 = np.zeros((ld2,d))\n",
    "\n",
    "\n",
    "for i in range(ld2):\n",
    "    img1 = dataset1[i].unsqueeze(0).to(device)\n",
    "    img2 = dataset2[i].unsqueeze(0).to(device)\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        enc1  = encoder(img1)\n",
    "        enc2  = encoder(img2)\n",
    "        img_feat1[i] = enc1.cpu().squeeze().numpy()\n",
    "        img_feat2[i] = enc2.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45ec84",
   "metadata": {},
   "source": [
    "## Sample clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33686ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMA Mods to save standard scaler\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "n_samples = img_feat1.shape[0]; #len(im_deep)\n",
    "n_tab = 20\n",
    "covar_1 = np.eye(n_tab)\n",
    "covar_1[2:5,8] = 0.5\n",
    "covar_1[8,2:5] = 0.5\n",
    "\n",
    "m1 = img_feat1[0:n_samples,:]\n",
    "m1 = scaler1.fit_transform(m1)\n",
    "\n",
    "m2 = img_feat2[0:n_samples,:]\n",
    "scaler2.fit(m2)\n",
    "m2 = scaler2.transform(m2)\n",
    "\n",
    "# sample both M3 and M4 together\n",
    "m3 = np.random.multivariate_normal(np.zeros(n_tab), covar_1, size=n_samples)\n",
    "labels = np.zeros((n_samples))\n",
    "\n",
    "# deep features\n",
    "features = np.concatenate((m1,m2,m3),1)\n",
    "\n",
    "# input data to the multimodal model\n",
    "multi_data = np.concatenate((dataset1[0:n_samples,:,:,:].squeeze().reshape(m1.shape[0],28*28).cpu().numpy(),\n",
    "                             dataset2[0:n_samples,:,:,:].squeeze().reshape(m2.shape[0],28*28).cpu().numpy(),m3),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea5dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fusion_features', features)\n",
    "np.save('inp_multi_data', multi_data)\n",
    "\n",
    "tr_num =  int(len(labels) - len(labels)*0.3)\n",
    "\n",
    "np.save('D1_train.npy',multi_data[0:tr_num,:])\n",
    "np.save('D1_fusion_train.npy', features[0:tr_num,:] )\n",
    "\n",
    "np.save('D1_test.npy',multi_data[tr_num:-1,:])\n",
    "np.save('D1_fusion_test.npy', features[tr_num:-1,:] )\n",
    "\n",
    "np.save('tr_num', tr_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
